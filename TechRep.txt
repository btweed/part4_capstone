As education budgets grow ever smaller, the need for effective identification of populations of greatest need has grown ever larger, so what funds remain available may do the most good. This project uses a demographics-based model to determine which groups of students perform best in their Common Core assessments.

Data collection
This project began with a dataset provided by a highly placed source in the New York City school system which compared the performance of charter schools on Common Core English and math assessments with that of the normal public schools in their same school districts, with the intention to pair it with demographic data at the ZIP code level from the US Census Bureau. Unfortunately, both the provided dataset and the available census data proved to be of insufficient granularity, and so had to be discarded. An alternate file was sourced from the New York State Department of Education website; this file had data for all Common Core assessments by school, as well as aggregated by geographic district, county and state. 

EDA
The first step was to perform standard data cleaning steps; I had to trim percent signs off the passing-rate columns, and the Department of Education in its infinite wisdom decided to represent missing values with ‘-‘ instead of ‘NaN’, so that needed replacing as well. Next, to avoid my head and computer exploding, I decided to limit the scope of this project to New York City, as had been originally intended. As each measurement had a “county” value regardless of level of aggregation, I was able to pull out anything that had to do with the five boroughs by selecting rows where “county” was New York, Kings, Queens, Bronx or Richmond. It seemed prudent to further separate by level of aggregation, so I split them into a county-level, district-level and school-level dataframe. I also set aside the statewide aggregated measurements at this point for potential comparison later.

Features/Fitting
Within the individual bucketing (i.e. school, district or county) the data was further subdivided into the demographic categories Male, Female, (Not) Economically Disadvantaged, (Not) Limited English Proficient, and several race categories. (There was also a category labeled Not Migrant, but no accompanying Migrant category so I ignored it.) I separated the features into opposing pairs (and the group of race labels), fitting the mean scaled score on each pair/group. 

Results/Further Work
Generally speaking, this was a bit of a bust. Gender had an r-squared of 0.01, economic status 0.03, race 0.2, and student disability and ESL each around 0.3. I took a punt on fitting student disability and ESL on the same model, and that got 0.2 as well. Plainly a more complex model is needed. There are an abundance of other score metrics available, such as rates of attainment for certain scores at certain grade levels, but I’d need to be able to work the dataframe as if it were a Rubik’s Cube, and I can’t do that yet. Hell, I can’t even do a Rubik’s Cube. In any event, though, this data has way more features than I was able to use, and there’s definitely a competent model in there somewhere.